{
 "metadata": {
  "name": "",
  "signature": "sha256:7429b1f5eb0d44c9dab9b046e916f8f0e10c189a89fd9258e7ae84b4412a749a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1234)\n",
      "import pymc3 as pm\n",
      "import scipy.stats as stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named nlinalg",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-4f279901533c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymc3\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/pymc3/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/pymc3/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcontinuous\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdiscrete\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmultivariate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/pymc3/distributions/multivariate.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdist_math\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named nlinalg"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "_logger = logging.getLogger(\"theano.gof.compilelock\")\n",
      "_logger.setLevel(logging.ERROR)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PyMC3\n",
      "----\n",
      "\n",
      "Install PyMC3 directly from GitHub with\n",
      "```\n",
      "pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3\n",
      "```\n",
      "\n",
      "- [Repository for PyMC3](https://github.com/pymc-devs/pymc3)\n",
      "- [Getting started](http://pymc-devs.github.io/pymc3/getting_started/)\n",
      "\n",
      "PyMC3 is alpha software that is intended to improve on PyMC2 in the following ways (from GitHub page):\n",
      "\n",
      "- Intuitive model specification syntax, for example, x ~ N(0,1) translates to x = Normal(0,1)\n",
      "- Powerful sampling algorithms such as Hamiltonian Monte Carlo\n",
      "- Easy optimization for finding the maximum a posteriori point\n",
      "- Theano features\n",
      "    - Numpy broadcasting and advanced indexing\n",
      "    - Linear algebra operators\n",
      "    - Computation optimization and dynamic C compilation\n",
      "- Simple extensibility\n",
      "\n",
      "It also comes with extensive [examples](https://github.com/pymc-devs/pymc3/tree/master/pymc3/examples) including ports of the R/JAGS code examples from [Doing Bayesian Data Analysis](https://github.com/aloctavodia/Doing_bayesian_data_analysis).\n",
      "\n",
      "However, the API is different and it is not backwards compartible with models specified in PyMC2."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Examples\n",
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Coin toss\n",
      "\n",
      "We'll repeat the example of determining the bias of a coin from observed coin tosses. The likelihood is binomial, and  we use a beta prior.\n",
      "\n",
      "Note the different API from PyMC2."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 100\n",
      "h = 61\n",
      "alpha = 2\n",
      "beta = 2\n",
      "\n",
      "niter = 1000\n",
      "with pm.Model() as model: # context management\n",
      "    # define priors\n",
      "    p = pm.Beta('p', alpha=alpha, beta=beta)\n",
      "    \n",
      "    # define likelihood\n",
      "    y = pm.Binomial('y', n=n, p=p, observed=h)\n",
      "    \n",
      "    # inference\n",
      "    start = pm.find_MAP() # Use MAP estimate (optimization) as the initial state for MCMC\n",
      "    step = pm.Metropolis() # Have a choice of samplers\n",
      "    trace = pm.sample(niter, step, start, random_seed=123, progressbar=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(trace['p'], 15, histtype='step', normed=True, label='post');\n",
      "x = np.linspace(0, 1, 100)\n",
      "plt.plot(x, stats.beta.pdf(x, alpha, beta), label='prior');\n",
      "plt.legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Estimating mean and standard deviation of normal distribution\n",
      "\n",
      "$$\n",
      "X \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate observed data\n",
      "N = 100\n",
      "_mu = np.array([10])\n",
      "_sigma = np.array([2])\n",
      "y = np.random.normal(_mu, _sigma, N)\n",
      "\n",
      "niter = 1000\n",
      "with pm.Model() as model:\n",
      "    # define priors\n",
      "    mu = pm.Uniform('mu', lower=0, upper=100, shape=_mu.shape)\n",
      "    sigma = pm.Uniform('sigma', lower=0, upper=10, shape=_sigma.shape)\n",
      "    \n",
      "    # define likelihood\n",
      "    y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=y)\n",
      "    \n",
      "    # inference\n",
      "    start = pm.find_MAP()\n",
      "    step = pm.Slice()\n",
      "    trace = pm.sample(niter, step, start, random_seed=123, progressbar=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(10,4))\n",
      "plt.subplot(1,2,1); \n",
      "plt.hist(trace['mu'][-niter/2:,0], 25, histtype='step');\n",
      "plt.subplot(1,2,2); \n",
      "plt.hist(trace['sigma'][-niter/2:,0], 25, histtype='step');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Estimating parameters of a linear regreession model\n",
      "\n",
      "We will show how to estimate regression parameters using a simple linear modesl\n",
      "\n",
      "$$\n",
      "y \\sim ax + b\n",
      "$$\n",
      "\n",
      "We can restate the linear model $$y = ax + b + \\epsilon$$ as sampling from a probability distribution\n",
      "\n",
      "$$\n",
      "y \\sim \\mathcal{N}(ax + b, \\sigma^2)\n",
      "$$\n",
      "\n",
      "Now we can use pymc to estimate the paramters $a$, $b$ and $\\sigma$ (pymc2 uses precision $\\tau$ which is $1/\\sigma^2$ so we need to do a simple transformation). We will assume the following priors\n",
      "\n",
      "$$\n",
      "a \\sim \\mathcal{N}(0, 100) \\\\\n",
      "b \\sim \\mathcal{N}(0, 100) \\\\\n",
      "\\tau \\sim \\text{Gamma}(0.1, 0.1)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# observed data\n",
      "n = 11\n",
      "_a = 6\n",
      "_b = 2\n",
      "x = np.linspace(0, 1, n)\n",
      "y = _a*x + _b + np.random.randn(n)\n",
      "\n",
      "with pm.Model() as model:\n",
      "    a = pm.Normal('a', mu=0, sd=20)\n",
      "    b = pm.Normal('b', mu=0, sd=20)\n",
      "    sigma = pm.Uniform('sigma', lower=0, upper=20)\n",
      "    \n",
      "    y_est = a*x + b # simple auxiliary variables\n",
      "    \n",
      "    likelihood = pm.Normal('y', mu=y_est, sd=sigma, observed=y)\n",
      "    # inference\n",
      "    start = pm.find_MAP()\n",
      "    step = pm.NUTS() # Hamiltonian MCMC with No U-Turn Sampler\n",
      "    trace = pm.sample(niter, step, start, random_seed=123, progressbar=True)\n",
      "    pm.traceplot(trace);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Alternative fromulation using GLM formulas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = dict(x=x, y=y)\n",
      "\n",
      "with pm.Model() as model:\n",
      "    pm.glm.glm('y ~ x', data)\n",
      "    step = pm.NUTS() \n",
      "    trace = pm.sample(2000, step, progressbar=True) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pm.traceplot(trace);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(7, 7))\n",
      "plt.scatter(x, y, s=30, label='data')\n",
      "pm.glm.plot_posterior_predictive(trace, samples=100, \n",
      "                                 label='posterior predictive regression lines', \n",
      "                                 c='blue', alpha=0.2)\n",
      "plt.plot(x, _a*x + _b, label='true regression line', lw=3., c='red')\n",
      "plt.legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Simple Logistic model\n",
      "\n",
      "We have observations of height and weight and want to use a logistic model to guess the sex."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# observed data\n",
      "df = pd.read_csv('HtWt.csv')\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "niter = 1000\n",
      "with pm.Model() as model:\n",
      "    pm.glm.glm('male ~ height + weight', df, family=pm.glm.families.Binomial()) \n",
      "    trace = pm.sample(niter, step=pm.Slice(), random_seed=123, progressbar=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note that height and weigth in trace refer to the coefficients\n",
      "\n",
      "df_trace = pm.trace_to_dataframe(trace)\n",
      "pd.scatter_matrix(df_trace[-1000:], diagonal='kde');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(12, 4))\n",
      "plt.subplot(121)\n",
      "plt.plot(df_trace.ix[-1000:, 'height'], linewidth=0.7)\n",
      "plt.subplot(122)\n",
      "plt.plot(df_trace.ix[-1000:, 'weight'], linewidth=0.7);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### There is no convergence!\n",
      "\n",
      "Becaue of ths strong correlation between height and weight, a one-at-a-time sampler such as the slice or Gibbs sampler will take a long time to converge. The HMC does much better."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "niter = 1000\n",
      "with pm.Model() as model:\n",
      "    pm.glm.glm('male ~ height + weight', df, family=pm.glm.families.Binomial()) \n",
      "    trace = pm.sample(niter, step=pm.NUTS(), random_seed=123, progressbar=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_trace = pm.trace_to_dataframe(trace)\n",
      "pd.scatter_matrix(df_trace[-1000:], diagonal='kde');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pm.summary(trace);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import seaborn as sn\n",
      "sn.kdeplot(trace['weight'], trace['height'])\n",
      "plt.xlabel('Weight', fontsize=20)\n",
      "plt.ylabel('Height', fontsize=20)\n",
      "plt.style.use('ggplot')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the logistic regression results to classify subjects as male or female based on their height and weight, using 0.5 as a cutoff, as shown in the plot below. Green = true positive male, yellow = true positive female, red halo = misclassification. Blue line shows the 0.5 cutoff."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "intercept, height, weight = df_trace[-niter//2:].mean(0)\n",
      "\n",
      "def predict(w, h, height=height, weight=weight):\n",
      "    \"\"\"Predict gender given weight (w) and height (h) values.\"\"\"\n",
      "    v = intercept + height*h + weight*w\n",
      "    return np.exp(v)/(1+np.exp(v))\n",
      "\n",
      "# calculate predictions on grid\n",
      "xs = np.linspace(df.weight.min(), df.weight.max(), 100)\n",
      "ys = np.linspace(df.height.min(), df.height.max(), 100)\n",
      "X, Y = np.meshgrid(xs, ys)\n",
      "Z = predict(X, Y)\n",
      "\n",
      "plt.figure(figsize=(6,6))\n",
      "\n",
      "# plot 0.5 contour line - classify as male if above this line\n",
      "plt.contour(X, Y, Z, levels=[0.5])\n",
      "\n",
      "# classify all subjects\n",
      "colors = ['lime' if i else 'yellow' for i in df.male]\n",
      "ps = predict(df.weight, df.height)\n",
      "errs = ((ps < 0.5) & df.male) |((ps >= 0.5) & (1-df.male))\n",
      "plt.scatter(df.weight[errs], df.height[errs], facecolors='red', s=150)\n",
      "plt.scatter(df.weight, df.height, facecolors=colors, edgecolors='k', s=50, alpha=1);\n",
      "plt.xlabel('Weight', fontsize=16)\n",
      "plt.ylabel('Height', fontsize=16)\n",
      "plt.title('Gender classification by weight and height', fontsize=16)\n",
      "plt.tight_layout();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Estimating parameters of a logistic model\n",
      "\n",
      "Gelman's book has an example where the dose of a drug may be affected to the number of rat deaths in an experiment.\n",
      "\n",
      "| Dose (log g/ml) | # Rats | # Deaths |\n",
      "|-----------------|--------|----------|\n",
      "| -0.896          | 5      | 0        |\n",
      "| -0.296          | 5      | 1        |\n",
      "| -0.053          | 5      | 3        |\n",
      "| 0.727           | 5      | 5        |\n",
      "\n",
      "We will model the number of deaths as a random sample from a binomial distribution, where $n$ is the number of rats and $p$ the probabbility of a rat dying. We are given $n = 5$, but we believve that $p$ may be related to the drug dose $x$. As $x$ increases the number of rats dying seems to increase, and since $p$ is a probability, we use the following model:\n",
      "\n",
      "$$\n",
      "y \\sim \\text{Bin}(n, p) \\\\\n",
      "\\text{logit}(p) = \\alpha + \\beta x \\\\\n",
      "\\alpha \\sim \\mathcal{N}(0, 5) \\\\\n",
      "\\beta \\sim \\mathcal{N}(0, 10)\n",
      "$$\n",
      "\n",
      "where we set vague priors for $\\alpha$ and $\\beta$, the parameters for the logistic model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# observed data\n",
      "n = 5 * np.ones(4)\n",
      "x = np.array([-0.896, -0.296, -0.053, 0.727])\n",
      "y = np.array([0, 1, 3, 5])\n",
      "\n",
      "def invlogit(x):\n",
      "    return pm.exp(x) / (1 + pm.exp(x))\n",
      "\n",
      "with pm.Model() as model:\n",
      "    # define priors\n",
      "    alpha = pm.Normal('alpha', mu=0, sd=5)\n",
      "    beta = pm.Flat('beta')\n",
      "    \n",
      "    # define likelihood\n",
      "    p = invlogit(alpha + beta*x)\n",
      "    y_obs = pm.Binomial('y_obs', n=n, p=p, observed=y)\n",
      "    \n",
      "    # inference\n",
      "    start = pm.find_MAP()\n",
      "    step = pm.NUTS()\n",
      "    trace = pm.sample(niter, step, start, random_seed=123, progressbar=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'pm' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-54c121eb0596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mpm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# define priors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'alpha'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'pm' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.exp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = lambda a, b, xp: np.exp(a + b*xp)/(1 + np.exp(a + b*xp))\n",
      "\n",
      "xp = np.linspace(-1, 1, 100)\n",
      "a = trace.get_values('alpha').mean()\n",
      "b = trace.get_values('beta').mean()\n",
      "plt.plot(xp, f(a, b, xp))\n",
      "plt.scatter(x, y/5, s=50);\n",
      "plt.xlabel('Log does of drug')\n",
      "plt.ylabel('Risk of death');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Using a hierarchcical model\n",
      "\n",
      "This uses the Gelman radon data set and is based off this [IPython notebook](http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/). Radon levels were measured in houses from all counties in several states. Here we want to know if the preence of a basement affects the level of radon, and if this is affected by which county the house is located in. \n",
      "\n",
      "The data set provided is just for the state of Minnesota, which has 85 counties with 2 to 116 measurements per county. We only need 3 columns for this example `county`, `log_radon`, `floor`, where `floor=0` indicates that there is a basement.\n",
      "\n",
      "We will perfrom simple linear regression on log_radon as a function of county and floor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "radon = pd.read_csv('radon.csv')[['county', 'floor', 'log_radon']]\n",
      "radon.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Hiearchical model\n",
      "\n",
      "With a hierarchical model, there is an $a_c$ and a $b_c$ for each county $c$ just as in the individual couty model, but they are no longer indepnedent but assumed to come from a common group distribution\n",
      "\n",
      "$$\n",
      "a_c \\sim \\mathcal{N}(\\mu_a, \\sigma_a^2) \\\\\n",
      "b_c \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)\n",
      "$$\n",
      "\n",
      "we furhter assume that the hyperparameters come from the following distributions\n",
      "\n",
      "$$\n",
      "\\mu_a \\sim \\mathcal{N}(0, 100^2) \\\\\n",
      "\\sigma_a \\sim \\mathcal{U}(0, 100) \\\\ \n",
      "\\mu_b \\sim \\mathcal{N}(0, 100^2) \\\\\n",
      "\\sigma_b \\sim \\mathcal{U}(0, 100)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "county = pd.Categorical(radon['county']).codes\n",
      "\n",
      "with pm.Model() as hm:\n",
      "    # County hyperpriors\n",
      "    mu_a = pm.Normal('mu_a', mu=0, tau=1.0/100**2)\n",
      "    sigma_a = pm.Uniform('sigma_a', lower=0, upper=100)\n",
      "    mu_b = pm.Normal('mu_b', mu=0, tau=1.0/100**2)\n",
      "    sigma_b = pm.Uniform('sigma_b', lower=0, upper=100)\n",
      "    \n",
      "    # County slopes and intercepts\n",
      "    a = pm.Normal('slope', mu=mu_a, sd=sigma_a, shape=len(set(county)))\n",
      "    b = pm.Normal('intercept', mu=mu_b, tau=1.0/sigma_b**2, shape=len(set(county)))\n",
      "    \n",
      "    # Houseehold errors\n",
      "    sigma = pm.Gamma(\"sigma\", alpha=10, beta=1)\n",
      "    \n",
      "    # Model prediction of radon level\n",
      "    mu = a[county] + b[county] * radon.floor.values\n",
      "    \n",
      "    # Data likelihood\n",
      "    y = pm.Normal('y', mu=mu, sd=sigma, observed=radon.log_radon)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with hm:\n",
      "    start = pm.find_MAP()\n",
      "    step = pm.NUTS(scaling=start)\n",
      "    hm_trace = pm.sample(2000, step, start=start, random_seed=123, progressbar=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8, 60))\n",
      "pm.forestplot(hm_trace, vars=['slope', 'intercept']);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}